{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Network MNIST fashion - exercise with validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network architecture\n",
    "# 784 input units (28x28 images)\n",
    "# 256 hiden layer -ReLU activation function\n",
    "# 128 hiden layer -ReLU activation function\n",
    "# 64 hiden layer -ReLU activation function\n",
    "# 10 output layer for each class (10 classes) - Log-softmax function\n",
    "# nn.Dropout module used to reduce overfitting, where input unit will be randomly dropped \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    def forward(self,x):\n",
    "        # Make sure that input tensor is flattened into 784 long vector\n",
    "        x = x.view(x.shape[0],-1)\n",
    "    \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        # no dropout for output\n",
    "        x = self.logsoftmax(F.relu(self.fc4(x)), dim=1)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During inference we want to use the entire network (no dropout).\n",
    "# Turn off dropout for: validation, testing, any predictions with model.eval() for which the dropout probability is 0\n",
    "# Turning on the dropout back with model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "# Definig the loss with the negative log likehood loss.(Loss - distance between the true and predicted values)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Checked also and loss is bigger for optim.SGD lr = 0.001 (for epoch 5 is 0.48467831648806775).\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "train_losses, test_losses = [], []\n",
    "# Training the network.\n",
    "for e in range(epochs):\n",
    "    running_loss = 0    \n",
    "    for labels, image in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(image)\n",
    "        \n",
    "        # Calculating the loss with the outputs and the labels.\n",
    "        loss = criterion(log_ps, labels)\n",
    "        \n",
    "        # Calculating the gradients for the parameters.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Taking an update step and few the new weights.\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss +=loss.item()\n",
    "        \n",
    "    else:\n",
    "        # Implementing the validation pass and printing out the validation accuracy.\n",
    "        # Goal of validation: measure the model performance on the test dataset - here as calculating the accuracy\n",
    "        # Turn off gradients - speeding up the code, as there will be no update of parameters in validation pass.\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad(): \n",
    "            # During inference the entire network is used, so no dropout\n",
    "            #model.eval() - sets the model to evaluation mode where dropout probability is 0\n",
    "            model.eval()\n",
    "            for labels, image in testloader:\n",
    "                log_ps = model(image)\n",
    "                test_loss =criterion(log_ps, labels)\n",
    "                running_loss_test +=loss.item()\n",
    "                \n",
    "                # Get the class probabilities\n",
    "                ps = torch.exp(log_ps)\n",
    "                \n",
    "                # Getting the most likely class using the ps.topk method - returns a tuple of the top-k values and the top-kindices\n",
    "                top_p, top_class = ps.topk(1,dim=1)\n",
    "                \n",
    "                #Check if the predicted classes match the labels\n",
    "                equals = top_class==labels.view(*top_class.shape)\n",
    "                \n",
    "                #calculate the percentage of correct predictions\n",
    "                #convert equals to a float tensor from torch.ByteTensor\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        #torch.mean returns a scalar tensor, to get the actual value as a float we'll need to do accuracy.item()\n",
    "        print(\"Training loss for epoch {} is {}\".format(e, running_loss/len(trainloader)),\n",
    "             \"Testing loss for epoch {} is {}\".format(e, test_loss/len(testloader)),\n",
    "             \"Accuraccy for epoch {} is {} %\".format(e. accuracy.item()/len(testloader)*100))        \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
